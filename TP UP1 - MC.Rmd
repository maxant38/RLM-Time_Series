---
title: 'TP UP1 - Maxence Caille'
author: "Maxence Caille"
output:
  word_document: default
---
#Q1)

**Import des données et rapide exploration**
```{r}
data <- read.table("Data_app.txt", header=TRUE)
names(data)
head(data)
summary(data)
```
**Tracer des 3 séries**
```{r}
plot(data)

par(mfrow=c(2,2))
plot(as.Date(data$date),data$kwh,col="blue" )
title("kwh au cours du temps")
plot(as.Date(data$date),data$cldd,col="red")
title("cldd au cours du temps (discret)")
plot(as.Date(data$date),data$htdd)
title("htdd au cours du temps")

plot(as.Date(data$date),data$kwh,col="blue" )
points(as.Date(data$date),data$cldd,col="red")
points(as.Date(data$date),data$htdd)
title("Superposition")

dataTemp <- ts(data, start = c(2006,1), frequency=12)
plot(dataTemp)

htdd <- dataTemp[,"htdd"]
cldd <- dataTemp[,"cldd"]
kwh <- dataTemp[,"kwh"]
date <-dataTemp[,"date"]

```


On constate que :
- Globalement la consommation en électricité est en augmentation de façon linéaire entre 2006 et 2020.Après une brève exploration des données, on constate que la consommation en electricité  est maximale annuelle est généralement lors du mois de juillet. 
- La demande en énergie pour climatiser est basse (souvent nulle) pendant les mois froids et est à son maximum pendant les mois chaud.
- A l'inverse la demande en énergie pour chauffer est à son maximum pendant les mois froids et est basse pendant les mois chaud.
La contraiment à la consommation en électricité, les valeurs htdd et cldd n'augment pas de façon linéaire au cours du temps. Par exemple, les valeurs maximales de cldd sont toutes entre 246 et 267 à partir de 2012 et la valeur minimale 0 est atteinte au moins une fois par an.
Les remarques précentes sont cohérentes avec la connaissance que nous avons du sujet. Il est logique que nous utilisons plus la climatisation pendant l'été et plus de chauffage pendant l'hivers. De même, on sait que notre consommation électrique augmente chaque années depuis plus de 20 ans (sauf peut être cet hivers ?!).

#Q2)


**Choix de notre modèle (relation entre les données temporelles exclues)**

***Définition d'un premier modèle***

Pour trouver notre modèle idéal nous allons procédé en utilisant la méthode "step wise": 
- On ajoute un nouveau prédicteur au modèle
- On regarde les résultats des test de Student pour chaque prédicteur
- Après réexamination, si des prédicteurs ne sont plus significatif (on décide dans ce tp que l'on rejettera l'hypothèse testée à 95% donc la p-valeur du test de Student doit être < 0,05 , on retire du modèle la moins significative d'entres elles.

Le processus s'arrête quand plus aucun prédicteur ne puisse être introduite ni retirer du modèle.

On considère ici que Etant donné que nous n'avons que deux variables(htdd et cldd) dans notre cas. Cette méthode est donc assez intuitive. Nous allons la réaliser de manière manuelle. Mais si nous avions eu beaucoup de variable nous aurions pu utiliser la fonction step() de R. 

```{r}


#on commence notre regression avec le prédicteur htdd uniquement 
lm0.fit <- lm(kwh ~ htdd  )

#On ajoute le prédicteur cldd
lm1.fit <- lm(kwh ~ htdd + cldd )

#On ajoute le prédicteur date
lm2.fit <- lm(kwh ~ date + htdd + cldd)

#On compare les trois modèles
summary(lm0.fit)
summary(lm1.fit)
summary(lm2.fit)

```
On constate que le modèle lm2.fit a une RMSE plus faible, un R^2 ajusté plus éleve et les prédicteurs sont tous considérés comme significatifs càd ont eu une p-value < 0,05 au test de student si on se place dans un test à 95% On voit que le prédicteur cldd bien plus significatif que htdd.Le modèle lm2.fit "passe" aussi le test du F1 (hypothèse nulle que tous les coefficients des prédicteurs sont nulles).
Le modèle lm2.fit est donc "meilleur" que le modèle lm1.fit et le modèle lm2.fit
En regardant les coefficients des prédicteurs. On voit que le prédicteur temps a un plus "fort impact" sur kwh que htdd et cldd. Cela veut dire que dans notre modèle la consommation électrique est plus impactée par une augmentation du prédicteur temps qu'une augmentation du chauffage/climatisation.

***Analyse en détail des résidues du modèle sélectionné***
```{r}
par(mfrow=c(2,2))
plot(lm2.fit)
```

Avec les graphiques produits avec les commandes précédentes on peut tirer les analyses suivantes.

**Scale-Location**
- Ce graphique nous permet d'évaluer une des hypothèses du  modèle de régresion linéaire : l'homoscédasticité. Si la droite rouge est horizontal cela veut que notre variance est constante. On peut voir que la droite n'est pas parfaitement horizontal , on a donc une légère situation d'hétéroscédasticité.

On peut tester cela avec le test de Breusch-Pagan:

```{r}
library(lmtest)
lmtest::bptest(lm2.fit)
```
L'interprétation du test de Breusch-Pagan pour l'hétéroscédasticité est simple. L'hypothèse nulle (H0) est : l'homostacité est présente. Comme la statistique de test (BP) est petite (=0.2873) et que la valeur p n'est pas significative (c'est-à-dire >0.05), nous ne rejetons pas l'hypothèse nulle (il n'y a pas d'évidence nette pour que H0 soit fausse). 

**Residual vs Fitted** : 
- si notre modèle de regression multiple modéliser parfaitement nos données (càd une realtion linéaire dans les données) on devrait avoir une ligne rouge droite. Or ce n'est pas le cas, il y a donc un pattern de non linéarité dans les données. 
- On peut ici aussi constater un léger phénomène d'hétéroscédacité.


**Residual vs Leverage**
- Ce graphique nous aide à identifier les points influents.Toutes les valeurs aberrantes ne sont pas influentes dans l'analyse de régression linéaire. En effet, même si les données présentent des valeurs extrêmes, elles peuvent ne pas être influentes pour déterminer une ligne de régression. Cela signifie que les résultats ne seraient pas très différents si nous les incluons ou les excluons de l'analyse. D'autre part, certains points pourraient être très influents même s'ils semblent se situer dans une fourchette raisonnable de valeurs. Ils peuvent être des cas extrêmes par rapport à une ligne de régression et peuvent modifier les résultats si nous les excluons de l'analyse. Dans notre cas, on ne voit même pas la ligne de Cook. Cela veut dire qu'il n'y a aucun points influents. 

Par précaution, on regarde les trois valeurs les plus influentes :
```{r}
data[c(7,168,163),]
```
On constate que ces valeurs ne sont pas abérantes.

**Normal Q-Q**
- Ce graphique montre si les résidus sont normalement distribués. On constate qu'il y a une très léger effet de "lighter tail". Cela signifie que par rapport à la distribution normale, il y a un peu plus de données situées aux extrêmes de la distribution et moins de données au centre de la distribution. Mais celui-ci reste très minime car la majorité des points collent bien à la droite du Q-Q plot.

```{r}
plot(lm2.fit$residuals)
```
On voit aussi que les résidus bruts sont influencé par leur index càd sont influencé par le temps étant donné que les index sont en fonction du temps.


***Résolution des problèmes identifiés***
Pour résoudre les problème identifiés précedement nous avons quelques solutions:
- Pour le problème d'hétéroscédasticité: On peut transformer la prédiction kwh avec une fonction concave com log() ou sqrt(). Ces fonctions réduisent de manière plus importante les reponses qui ont une valeur élevées, ce qui réduit l'hétéroscédascité.
- Pour résoudre le problème non linéarité des données. On peut appliquer des transformations non linéaires (X^2 , srqt(X), log X...) à nos prédicteurs. Au vu de la forme en forme de U de la droite rouge du plot fitted vs residuals. La transformation la plus pertinente me semble être un X^2.


Suite aux problèmes identifiés, j'ai testé d'autres modèle de regression linéaire multiples. La méthodologie pour les analyser/comparer est la même que celle qui a été utilisée pour "lm2.fit".




Les modèles testés ne sont pas meilleurs et sont plus complexe que lm2.fit. Nous allons pouvoir créer un meilleur modèle en ajoutant la temporalité.




Création du jeu de donner pour entrainer nos modèles et celui pour les tester. Je décide de séparer mon jeu au niveau de l'année 2019.
```{r}

dataTrainBrut <- dataTemp[1:156,]
dataTestBrut <- dataTemp[157:168,]
dataTrain <- ts(dataTrainBrut, start = c(2006,1), frequency=12)
dataTest <- ts(dataTestBrut, start = c(2019,1), frequency=12)
yTest <- dataTemp[157:168,2]
plot(dataTrain)
plot(dataTest)


```


**Creation fonction RMSE pour évaluer nos modèles**

```{r}
#la fonction attend comme paramètre la somme des résidus. Dans notre cas , ils sont accessibles via la commande modele_regression$residuals
compute_rmse <- function(x){
  sqrt(mean(x^2))
}

```


Je teste les différentes améliorations suggérées précédmeent (transofmation ^2 , sqrt, log). Et je compare les modèles en calculant le RMSE/analyse du summary.
```{r}

lm2.fit <- lm(kwh ~ date + htdd + cldd, data = dataTrain)
yEstime2 <- predict(lm2.fit, newdata = dataTest)
residus2 <- yTest-yEstime2
compute_rmse(residus2)

lm3.fit <- lm(kwh ~ htdd + date + cldd + I(date^2), data= dataTrain )
yEstime3 <- predict(lm3.fit, newdata = dataTest)
residus3 <- yTest-yEstime3
compute_rmse(residus3)

lm4.fit <- lm(log(kwh) ~ htdd + date + cldd + I(date^2), data= dataTrain )
yEstime4 <- predict(lm4.fit, newdata = dataTest)
yEstime4 <- exp(yEstime4)
residus4 <- yTest-yEstime4
compute_rmse(residus4)

lm5.fit <- lm(sqrt(kwh) ~ htdd +  date + cldd  + I(date^2) , data= dataTrain )
yEstime5 <- predict(lm5.fit, newdata = dataTest)
yEstime5 <- yEstime5^2
residus5 <- yTest-yEstime5
compute_rmse(residus5)


lm6.fit <- lm(kwh ~ htdd + date + cldd  + sqrt(date)+ I(date^2), data= dataTrain )
yEstime6 <- predict(lm6.fit, newdata = dataTest)
residus6 <- yTest-yEstime6
compute_rmse(residus6)

summary(lm2.fit)
summary(lm3.fit)
summary(lm5.fit)
summary(lm5.fit)
summary(lm6.fit)



```
D'après les résultats, lm.fit3/lm5.fit/lm6.fit ont les meilleures RMSE. Mais lm6.fit a deux de ces prédicteurs qui ne sont pas significatifs.

```{r}
summary(lm5.fit)
plot(lm5.fit)

summary(lm3.fit)
plot(lm3.fit)
```
lm5.fit :

On voti que tous les prédicteurs sont significatifs. Le F-statistic est très haut. On constate aussi que sur le graphe Fitted vs Residuals, notre modèle est meilleur car la "droite rouge" est bien horizontale (vs la forme en U du mdèle lm2.fit).La seule chose qui est "moins" bien est l'effet "lighter tail" qui est un peu plus accentué (vs lm2.fit).

lm3.fit:
De même pour le graphique Fitted vs Residuals/prédicteurs significatifs. Par contre les données sont "mieux" normalement distribué car elles suivent plus le graphique qq-plot.


*Q4*



```{r}

dataTempTwo <- data[, 2]
dataSeriesTwo <- ts(data=dataTempTwo, start = c(2006,1), freq=12 )

acfSerie <- acf( as.vector(dataSeriesTwo), lag=20, ylim = c(-1,1), 
          main = expression(paste("ACF empirique de la série ", y[t])), xlab="Décalage h", lwd=2)

pacfSerie <- pacf(
  
   as.vector(dataSeriesTwo), lag=20, ylim = c(-1,1), 
          main = expression(paste("PACF empirique de la série ", y[t])), xlab="Décalage h", lwd=2
)

```
Je constate que la série n'est pas stationnaire car les valeurs sur le graphique de l'ACF ne décroissent pas.
Une façon de déterminer plus objectivement si la différenciation est nécessaire est d'utiliser un test de racine unitaire.
Je fais un test de Dickey-Fuller augmenté. 

```{r}
library(tseries)
adf.test(dataTempTwo)

```
On voit qu la p-valeur associée au test de Dicker-Fuller augmenté est 0.3065 . Par conséquent, on ne peut pas rejeter l'hypothèse null H0 (car p-valeur>0.05). La série est bien "non stationnaire".

Je vais stationnariser la série en faisant une différenciation simple.

```{r}

diffdataSeriesTwo<- diff(dataSeriesTwo)
mu <- mean(diffdataSeriesTwo)
op <- par(mfrow = c(2,1), mex=0.9)
plot(diffdataSeriesTwo, type='o', main = expression(paste("Série différenciée ")),
     xlab="Temps", ylab = expression(y[t]))
abline(h=mu, col="red", lwd=2)
plot(dataSeriesTwo, type='l', xlab="Temps", ylab="kwh",
     main="Série initiale kwh",cex.main=1)
grid()


```
On peut refaire un test de Dicker-Fuller augmenté sur cette série différenciée.

```{r}


adf.test(diffdataSeriesTwo)

```
La p-valeur est >0.05, on rejette H0 : la série est bien stationnaire.

```{r}
lag.plot(diffdataSeriesTwo,12, layout=c(2,6), diag.col="red")
```
On observe de fortes variations saisonnières et une autocorrélation forte au lag 12, ce qui m'a orienté à utiliser un modèle saisonnier


```{r}

acfSerieDiff <- acf( as.vector(diffdataSeriesTwo), lag=50, ylim = c(-1,1), 
          main = expression(paste("ACF empirique de la série ")), xlab="Décalage h", lwd=2)
pacfSerieDiff <- pacf(
  
   as.vector(diffdataSeriesTwo), lag=50, ylim = c(-1,1), 
          main = expression(paste("PACF empirique de la série ")), xlab="Décalage h", lwd=2
)


```

La fonction d’autocorrélation présente des pics significatifs pour le premier retard et pour les retards aux pas multiples de 12. La fonction d’autocorrélation partielle une décroissance sinusoïdale amortie. Nous pouvons donc penser à un modèle du type SARIMA(p,d,q)(P,D,Q)[S]

S = 12 car on a une sésonnalité d'un an (cf graphe lag)
p =0 
d=1 car pour obtenir la serie differencie nous avons juste fait un difference de deux termes consécutifs. 
Comme les pics aux retards multiples de 12 ne semblent pas décroître exponentiellement je suppose que D != 0.Pour éviter de considérer des modèles trop complexes, il est généralement conseillé de satisfaire la condition d+D ≤ 2 (c’est-à-dire nous restreindre ici au cas D = 1).

On a donc le modèle suivant : SARIMA(p,1,q)(P,1,Q)[12]. Pour définir les paramètres p,q,P et Q, je vais préférer utiliser la fonction auto.arima qui va tester les différents modèles candidates. Il aurait été possible d'analyser par moi même les différents modèles via des PACF/ACF et comparer les modèles en essayant de minimiser un critère comme l'AIC ou l'AICc ou le BIC. Mais pour garder le compte rendu succinct je vais utiliser cette méthode automatisé.



```{r}
library(forecast)
sarimaModel <- auto.arima(dataTrain[,2], D= 1, d=1, approximation = FALSE, seasonal = TRUE)
summary(sarimaModel)
myforecasts <- forecast::forecast(sarimaModel, h=12) 
yEstime7 <- myforecasts$mean
residus7 <- yTest-yEstime7
compute_rmse(residus7)
```

On obtient donc un modèle SARIMA(0,1,2)(0,1,1)[12]. Sa RMSE est plus élevée que celle obtenue avec le modèle lm5.fit. 


Je vais essayer de combiner les résultats de ces deux modèles.Je mets un poids plus important pour les modèles de regression linéaire car leur RMSE était plus faible.


```{r}


yEstimeFinal <- ((yEstime3 + yEstime5)*2 + yEstime7 )/5
residus7 <- yTest - yEstimeFinal
compute_rmse(residus7)


```
On obtient un RMSE très faible : 4.9 . 

**Modèle final**

On entraine notre modèle final:

```{r}
#load des données er séparation des jeux de donnees


data.app.brut <- read.table("Data_app.txt", header=TRUE)
data.app <- data.app.brut[,-2]
data.test <- read.table("Data_test.txt", header=TRUE)
data.final <- rbind(data.app, data.test)

#pour modèle regression multiples
data.final.ts.train <- ts(data.app.brut, start = c(2006,1), frequency=12)
data.final.ts <- ts(data.final, start = c(2006,1), frequency=12)
data.final.ts.test.brut <- data.final.ts[169:180,]
data.final.ts.test <- ts(data.final.ts.test.brut, start = c(2006,1), frequency=12)


#pour modèle sarima
data.final.train.sarima <- ts(data.app.brut[,2], start = c(2006,1), frequency=12)


#training
lm3.fit <- lm(kwh ~ htdd + date + cldd + I(date^2), data= data.final.ts.train)
lm5.fit <- lm(sqrt(kwh) ~ htdd +  date + cldd  + I(date^2) , data= data.final.ts.train )
sarimaModelFinal <- auto.arima(data.final.train.sarima, D= 1, d=1, approximation = FALSE, seasonal = TRUE)


#prediction
yEstimefinal.3 <- predict(lm3.fit, newdata = data.final.ts.test)
yEstimefinal.5 <- predict(lm5.fit, newdata = data.final.ts.test)
yEstimefinal.5  <- yEstimefinal.5^2
forecasts.final <- forecast::forecast(sarimaModelFinal, h=12) 
yEstimefinal.7 <- forecasts.final$mean

yPreditfinal <- ((yEstimefinal.3+ yEstimefinal.5)*2 + yEstimefinal.7)/5


```


```{r}

print(yEstimefinal.3)
print(yEstimefinal.5)
print(yEstimefinal.7)
print(yPreditfinal)
```

